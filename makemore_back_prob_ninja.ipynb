{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba3f4d3-8620-4b45-8048-c3736ae6e0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb197ce6-d904-4448-9bf9-42c8d102d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "if Path('names.txt').exists():\n",
    "    words = open('names.txt', 'r').read().splitlines()\n",
    "else:\n",
    "    req = requests.get(r'https://raw.githubusercontent.com/karpathy/makemore/master/names.txt')\n",
    "    with open('names.txt', 'wb') as f:\n",
    "        f.write(req.content)\n",
    "    words = open('names.txt','r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59323a6c-429b-4031-a70f-44df451bcc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a973c3c-b120-4028-bd6f-be1458754e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf2feccd-1a49-43bc-8b03-b868c5548c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182546, 3]) torch.Size([182546])\n",
      "torch.Size([22840, 3]) torch.Size([22840])\n",
      "torch.Size([22760, 3]) torch.Size([22760])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(2147483647)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e750c59-b11b-4858-aafc-77d78b30f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function which we will use later when copamring Manual Gradients to Pytorch Gradients\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f\"{s:15s} | exact: {str(ex):5s} | approximate {str(app):5s} | maxdiff: {maxdiff}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b1fea37-5021-4dee-9d40-6990fa36f0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 64\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd), generator = g)\n",
    "\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator= g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden, generator = g) * 0.1  # using b1 just for fun, it's useless because of BN\n",
    "\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator = g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator = g) * 0.1\n",
    "\n",
    "#BatchNorm Parameters\n",
    "bngain = torch.randn((1, n_hidden), generator = g)*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden), generator = g) * 0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "733b5ec5-723d-4e32-b94c-257488925e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size #a shorter variable for convinence\n",
    "#constrcut a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0],(batch_size,), generator = g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a96e3615-cec5-4a73-9193-e2a4264f4952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4947, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Forward pass,k \"chunckated into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into the vector\n",
    "embcat = emb.view(emb.shape[0], -1) #Concatenate the vectors\n",
    "\n",
    "#liner layer 1 \n",
    "hprebn = embcat @ W1 + b1 #hidden layer pre activation\n",
    "#Batch Norm Layer\n",
    "bnmeani= 1/n*hprebn.sum(0,keepdim = True) #(hprebn.sum(0,keepdim = True)/n)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff **2\n",
    "bnvar = 1.0/(n-1)*(bndiff2).sum(0, keepdim= True) #note : Bessel's Correction (dividing by n-1 , not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw +bnbias\n",
    "\n",
    "#Non Linearity\n",
    "h = torch.tanh(hpreact) #hidden layer\n",
    "\n",
    "#linear layer 2\n",
    "logits = h @ W2 + b2 #Output Layer\n",
    "\n",
    "#cross entropy loss (same as F.cross_Entropy loss)\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes #subtract the max for numerical stability refer the previous notebooks \n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims = True)\n",
    "counts_sum_inv = counts_sum ** -1  # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "#pytorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24ad3381-dc2b-4175-a17e-bbffeb44ef2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate True  | maxdiff: 0.0\n",
      "bndiff          | exact: False | approximate False | maxdiff: 0.0010094988392665982\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "dprobs = (1.0 / probs) * dlogprobs\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "dnorm_logits = counts * dcounts\n",
    "dlogits = dnorm_logits.clone()\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "\n",
    "# print(h)\n",
    "    \n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "# cmp('bnvar', dbnvar, bnvar)\n",
    "# cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "# cmp('bnmeani', dbnmeani, bnmeani)\n",
    "# cmp('hprebn', dhprebn, hprebn)\n",
    "# cmp('embcat', dembcat, embcat)\n",
    "# cmp('W1', dW1, W1)\n",
    "# cmp('b1', db1, b1)\n",
    "# cmp('emb', demb, emb)\n",
    "# cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06805db2-3af2-4e9b-9ea9-209606f6cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea54ba48-79b7-4a00-8fb6-70f55f3560e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs shape torch.Size([32, 27])\n",
      "Calculating the gradients for ech parameters or steps which will be used in back propagation\n",
      "\n",
      "the gradients that need to be updated for a step are calculated by finding the derivative from the succeeding step and\n",
      "will be stored in the parameter.grad \n",
      " \n",
      "since the most part of the logprobs which has the shape of torch.Size([32, 27]) is gonna be zero     \n",
      "Because only the logprobs[range(n), Yb] of shape torch.Size([32]) will be taken into consideration\n",
      "      and loss of other logits or logprobs will be derviatively - d/dx zero\n",
      "\n",
      "-----Implementing the derivative-----\n",
      "logprobs        | exact: True  | approximate True  | maxdiff: 0.0\n",
      "\n",
      "\n",
      "derivative of probs.log() is equivalent to log(x) which is 1/x and need to implement chain rule      \n",
      "that is wee need to find the derivative of probs which is dlogprobs\n",
      "Compare the gradients probs.grad[range(n), Yb] and dprobs[range(n), Yb]\n",
      "(1.0/probs) * dlogprobs  this will boost the gradient if the predicted logits / probs of the actual value is low and\n",
      "will remain the same if the prob is One\n",
      "probs           | exact: True  | approximate True  | maxdiff: 0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cmp('logprobs', dlogprobs, logprobs)\n",
    "print('logprobs shape',logprobs.shape)\n",
    "#calcualte the dlogprobs \n",
    "#example loss = -(a + b + c) /3 where 3 is the n \n",
    "#dloss/da = -1/3 where b and c are constatnts and will become zero so the derivative loss for n logits can be written as -1/n\n",
    "print(f\"Calculating the gradients for ech parameters or steps which will be used in back propagation\\n\")\n",
    "print(\"the gradients that need to be updated for a step are calculated by finding the derivative from the succeeding step and\\\n",
    "\\nwill be stored in the parameter.grad \\n \")\n",
    "print(f\"since the most part of the logprobs which has the shape of {logprobs.shape} is gonna be zero\\\n",
    "     \\nBecause only the logprobs[range(n), Yb] of shape {logprobs[range(n), Yb].shape} will be taken into consideration\\n \\\n",
    "     and loss of other logits or logprobs will be derviatively - d/dx zero\")\n",
    "\n",
    "print(\"\\n-----Implementing the derivative-----\")\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "#1.0/n is the equivalent of dloss/dlogprobs\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "print(\"\\n\")\n",
    "\n",
    "# cmp('probs', dprobs, probs)\n",
    "# now find the gradient of probs which will the derivative from the logprob step\n",
    "print(f\"derivative of probs.log() is equivalent to log(x) which is 1/x and need to implement chain rule\\\n",
    "      \\nthat is wee need to find the derivative of probs which is dlogprobs\")\n",
    "print(\"Compare the gradients probs.grad[range(n), Yb] and dprobs[range(n), Yb]\")\n",
    "dprobs = (1.0/probs) * dlogprobs \n",
    "##take the derivative of the log(probs) and multiply it with the corresponding function's output i.e chain rule\n",
    "print('(1.0/probs) * dlogprobs  this will boost the gradient if the predicted logits / probs of the actual value is low and\\\n",
    "\\nwill remain the same if the prob is One')\n",
    "cmp('probs',dprobs,probs)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "## cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5b30694-5f35-48fa-b6e0-0ac54b42ee2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6881, -2.6308, -1.3207, -0.7717, -2.6308, -2.0945, -2.0945, -2.5493,\n",
       "        -1.7972, -0.7673, -0.7849, -0.5879, -0.8739, -0.6596, -0.5035, -0.9642,\n",
       "        -0.3914, -0.5605, -1.2179, -1.6703, -0.5830, -0.5615, -1.0826, -0.7308,\n",
       "        -1.5323, -0.6393, -1.7172, -0.7171, -2.0299, -1.5837, -0.5114, -1.1402])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.grad[range(n), Yb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28202500-bbcd-4e93-988f-da5e8985f7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6881, -2.6308, -1.3207, -0.7717, -2.6308, -2.0945, -2.0945, -2.5493,\n",
       "        -1.7972, -0.7673, -0.7849, -0.5879, -0.8739, -0.6596, -0.5035, -0.9642,\n",
       "        -0.3914, -0.5605, -1.2179, -1.6703, -0.5830, -0.5615, -1.0826, -0.7308,\n",
       "        -1.5323, -0.6393, -1.7172, -0.7171, -2.0299, -1.5837, -0.5114, -1.1402],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dprobs[range(n), Yb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b7bb739-7667-44c6-8685-707ffb14ec8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2641],\n",
       "        [-0.3696],\n",
       "        [-0.2916],\n",
       "        [-0.4176],\n",
       "        [-0.3696],\n",
       "        [-0.3349],\n",
       "        [-0.3349],\n",
       "        [-0.3644],\n",
       "        [-0.3583],\n",
       "        [-0.3276],\n",
       "        [-0.2530],\n",
       "        [-0.3644],\n",
       "        [-0.3897],\n",
       "        [-0.2012],\n",
       "        [-0.4114],\n",
       "        [-0.4074],\n",
       "        [-0.3914],\n",
       "        [-0.2931],\n",
       "        [-0.3724],\n",
       "        [-0.2381],\n",
       "        [-0.3597],\n",
       "        [-0.4602],\n",
       "        [-0.3644],\n",
       "        [-0.2390],\n",
       "        [-0.3929],\n",
       "        [-0.4929],\n",
       "        [-0.2022],\n",
       "        [-0.3380],\n",
       "        [-0.3295],\n",
       "        [-0.3644],\n",
       "        [-0.3829],\n",
       "        [-0.3422]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_sum_inv.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2283206c-e87b-45d1-8e48-5e26b00c645a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 27]) torch.Size([32, 1])\n",
      "the shape is not matched so it will braod cast the tensors/ logits that is the 32 columns will be broadcasted 27 times \n",
      "If we want to take the derivative of 2 fns we have to take them with respect to each other\n",
      "dcount_sum_inv shape is torch.Size([32, 1])\n",
      "counts_sum_inv  | exact: True  | approximate True  | maxdiff: 0.0\n",
      "\n",
      "we are not checking the gradienst yet because the we calculated only the first contribution of counts\n",
      "`probs = counts * counts_sum_inv` we have another contribution from the counts `counts_sum = counts.sum(1, keepdims=True)`\n",
      "dcounts_sum.shape = torch.Size([32, 1])\n",
      "counts_sum      | exact: True  | approximate True  | maxdiff: 0.0\n",
      "counts.shape torch.Size([32, 27]), counts_sum.shape torch.Size([32, 1])\n",
      "counts          | exact: True  | approximate True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "## cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "# counts_sum_inv is 1/counts_sum\n",
    "print(counts.shape, counts_sum_inv.shape)\n",
    "print(\"the shape is not matched so it will braod cast the tensors/ logits that is the 32 columns will be broadcasted 27 times \")\n",
    "\n",
    "# c = a * b\n",
    "# a[3 x 3] * b[3x1]\n",
    "# a11*b1 a12*b1 a13*b1\n",
    "# a21*b2 a22*b2 a23*b2\n",
    "# a31*b3 a32*b3 a33*b3\n",
    "\n",
    "print(\"If we want to take the derivative of 2 fns we have to take them with respect to each other\")\n",
    "# da*b/da = b    ,  da*b/db = a  and multiple the chain rule of that particular function\n",
    "#counts * dprobs will be of shape 32 * 27 but we nned 32 * 1 \n",
    "# because thats the shape of counts_sum_inv and we are doing it by adding all the gradients across the rows\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdims = True)\n",
    "print(f\"dcount_sum_inv shape is {dcounts_sum_inv.shape}\")\n",
    "\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "\n",
    "# cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "#Now we have to derivate with respect to counts now\n",
    "\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "\n",
    "print(\"\\nwe are not checking the gradienst yet because the we calculated only the first contribution of counts\\n\\\n",
    "`probs = counts * counts_sum_inv` we have another contribution from the counts `counts_sum = counts.sum(1, keepdims=True)`\")\n",
    "\n",
    "# derivatives of counts_sum**-1 can be writen as d(1/x)/dx = -1/x**2\n",
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
    "print(f'dcounts_sum.shape = {dcounts_sum.shape}')\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "\n",
    "#derivatives of counts.sum(1, keepdims=True)\n",
    "print(f\"counts.shape {counts.shape}, counts_sum.shape {counts_sum.shape}\")\n",
    "\n",
    "#a11 a12 a13 ----> b1 (= a11 + a12 + a13)\n",
    "#a21 a22 a23 ----> b2 (= a21 + a22 + a23)\n",
    "#a31 a32 a33 ----> b1 (= a31 + a32 + a33)\n",
    "#the derivatives of b1 sould be distributed equally across its respective inputs that is a11 a12 a13 .... applied similarly to all\n",
    "# we have to create a 32 x 27 from 32 x 1\n",
    "#we have to add the dcounts to the first branch also\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "# dcounts -> output\n",
    "# tensor([[17.9747, 17.9747, 17.9747, 17.9747, 17.9747, 17.9747, 17.9747, 17.9747,\n",
    "#          17.9747, 17.9747, 17.9747, 17.9747, 17.9747, 17.9747, 17.9747, 17.9747,\n",
    "#          17.9747, 17.9747, 17.9747, 17.9747, 17.9747, 17.9747, 17.9747, 17.9747,\n",
    "#          17.9747, 17.9747, 17.9747],\n",
    "#         ...................\n",
    "# after adding the branchs of counts\n",
    "# tensor([[17.9747, 17.9747, 17.9747, 17.9747, 17.9747, 17.9747, 17.9747, 17.9747,\n",
    "#          17.9747, 17.9747, 17.9747, 17.9747, 17.9747, 17.9747, 17.9747, 17.9747,\n",
    "#          17.9747, 17.9747, 17.9747, 17.9747, 17.8570, 17.9747, 17.9747, 17.9747,\n",
    "#          17.9747, 17.9747, 17.9747],\n",
    "#         ...........\n",
    "# dcounts\n",
    "cmp('counts', dcounts, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb80b170-7a56-47c3-8aba-cdcabe183050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_logits     | exact: True  | approximate True  | maxdiff: 0.0\n",
      "shape of dnorm_logits torch.Size([32, 27]) | shape of logits torch.Size([32, 27])| shape of logit_maxes torch.Size([32, 1])\n",
      "logit_maxes     | exact: True  | approximate True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "# local derivative of e**x is e**x\n",
    "\n",
    "dnorm_logits = counts * dcounts  #applied chain rule\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "\n",
    "# cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "#norm_logits = logits - logit_maxes\n",
    "print(f\"shape of dnorm_logits {dnorm_logits.shape} | shape of logits {logits.shape}\\\n",
    "| shape of logit_maxes {logit_maxes.shape}\")\n",
    "\n",
    "dlogits = dnorm_logits.clone()\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim =True)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "dlogits.shape\n",
    "\n",
    "# cmp('logits', dlogits, logits)\n",
    "# logit_maxes  = logits.max(1, keepdim=True).values\n",
    "\n",
    "dlogits += F.one_hot(logits.max(1).indices , num_classes = logits.shape[1]) * dlogit_maxes\n",
    "dlogits\n",
    "cmp('logits', dlogits, logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33d95be0-dcf3-4a09-b3b3-4889d3cf47ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23904fccb90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(F.one_hot(logits.max(1).indices , num_classes = logits.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7a3e41e-9a47-4f59-888c-8ec8421c95d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of dlogits - torch.Size([32, 27]) \n",
      "shape of h - torch.Size([32, 64]) \n",
      "shape of W2 - torch.Size([64, 27]) \n",
      "shape of b2 - torch.Size([27])\n",
      "h               | exact: True  | approximate True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# cmp('h', dh, h)\n",
    "# logits = h @ W2 + b2\n",
    "\n",
    "#we have to derivate through h @ W2 + b\n",
    "# Find the derivation in the notebook for simple problem and understand it and latter apply it for the broader picture \n",
    "# Tip to know the matrix multiplication for these parameter is \"Always look for the shape of the shapes of them\"\n",
    "# the corresponding shape of the derivative should be equal to the original shape like dh.shape == h.shape\n",
    "\n",
    "print(f\"shape of dlogits - {dlogits.shape} \\n\\\n",
    "shape of h - {h.shape} \\nshape of W2 - {W2.shape} \\nshape of b2 - {b2.shape}\")\n",
    "dh = dlogits @ W2.T\n",
    "cmp('h', dh, h)\n",
    "\n",
    "\n",
    "# cmp('W2', dW2, W2)\n",
    "dW2 = h.T @ dlogits\n",
    "cmp('W2', dW2, W2)\n",
    "\n",
    "# cmp('b2', db2, b2)\n",
    "db2 = dlogits.sum(0)\n",
    "cmp('b2', db2, b2)\n",
    "\n",
    "# cmp('hpreact', dhpreact, hpreact)\n",
    "# the derivative of tanh(x) is sech^2(x) and sech**2x = 1- tanh**2\n",
    "\n",
    "# h = torch.tanh(hpreact)\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "cmp('hpreact', dhpreact, hpreact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d99473e-49e3-4186-9f4f-57ab877a54d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapoe of dhpreacttorch.Size([32, 64]) \n",
      "shape of bngain torch.Size([1, 64]) \n",
      "shape of bnraw torch.Size([32, 64]) \n",
      "shape of bnbias torch.Size([1, 64])\n",
      "bngain          | exact: True  | approximate True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# cmp('bngain', dbngain, bngain)\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "# Lets see the shape of dhpreat, bngain, bnraw ,bnbias\n",
    "\n",
    "print(f\"shapoe of dhpreact{dhpreact.shape} \\n\\\n",
    "shape of bngain {bngain.shape} \\nshape of bnraw {bnraw.shape} \\n\\\n",
    "shape of bnbias {bnbias.shape}\")\n",
    "\n",
    "#hpreact = bngain * bnraw + bnbias are the element wise multiplication , Not Mat Mul , SO they are gonna be simple derivatives\n",
    "# Tip look for the shapes of the local derivastive and chain rule and act accordingly\n",
    "# bdnraw * dhpreact gonna be 32 x 64 , But we need it to be in the shape of bngain 1 x 64\n",
    "# if the function has 1 in its dimension use keepdims = True else Not Needed\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdims = True)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "\n",
    "#Similarly \n",
    "# cmp('bnraw', dbnraw, bnraw)\n",
    "\n",
    "dbnraw = bngain * dhpreact\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "\n",
    "# cmp('bnbias', dbnbias, bnbias)\n",
    "dbnbias = dhpreact.sum(0, keepdims= True)\n",
    "cmp('bnbias', dbnbias, bnbias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84dba147-6d9b-4dc7-92ec-faed31d4dd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of dbnraw torch.Size([32, 64]) | shape of bndiff torch.Size([32, 64]) | shape of bnvar_inv torch.Size([1, 64])\n",
      "Broad Casting is happening because of the shape of bnvar_inv\n",
      "dbnvar_inv      | exact: True  | approximate True  | maxdiff: 0.0\n",
      "dbndiff         | exact: False | approximate False | maxdiff: 0.0010094988392665982\n",
      "bnvar           | exact: True  | approximate True  | maxdiff: 0.0\n",
      "shape of bnvar torch.Size([1, 64]) | shape of dbnvar torch.Size([1, 64]) | shape of bndiff2 torch.Size([32, 64])\n",
      "bndiff2         | exact: True  | approximate True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate True  | maxdiff: 0.0\n",
      "shape of bndiff torch.Size([32, 64]) | shape of hprebn torch.Size([32, 64]) | shape of bnmeani torch.Size([1, 64])\n",
      "bnmeani         | exact: True  | approximate True  | maxdiff: 0.0\n",
      "hprebn          | exact: False | approximate False | maxdiff: 0.0022019059397280216\n",
      "hprebn          | exact: True  | approximate True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Batch Norm Layer\n",
    "# bnmeani= 1/n*hprebn.sum(0,keepdim = True) #(hprebn.sum(0,keepdim = True)/n)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff **2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim= True) #note : Bessel's Correction (dividing by n-1 , not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff - bnvar_inv\n",
    "# hpreact = bngain * bnraw +bnbias\n",
    "\n",
    "print(f\"shape of dbnraw {dbnraw.shape} | shape of bndiff {bndiff.shape} |\\\n",
    " shape of bnvar_inv {bnvar_inv.shape}\")\n",
    "print(f\"Broad Casting is happening because of the shape of bnvar_inv\")\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdims = True)\n",
    "cmp('dbnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "\n",
    "# cmp('dbndiff', dbndiff, bndiff)\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "cmp('dbndiff', dbndiff, bndiff)\n",
    "\n",
    "# cmp('bnvar', dbnvar, bnvar)\n",
    "# d (x ** -n ) / dx is -n * x ** -n-1\n",
    "dbnvar = (-0.5) * (bnvar + 1e-5) ** -1.5 * 1 * dbnvar_inv # where 1 is the ans of derivative of (bnvar + 1e-5) w.r.t bnvar\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "\n",
    "# cmp('bndiff2', dbndiff2, bndiff2)\n",
    "print(f\"shape of bnvar {bnvar.shape} | shape of dbnvar {dbnvar.shape} |\\\n",
    " shape of bndiff2 {bndiff2.shape}\")\n",
    "\n",
    "#the derivativce should be of bndiff2 shape as they are squashed to 1 x 64 in the forward pass and now they should be \n",
    "# distributed to 32 while back propagating\n",
    "\n",
    "dbndiff2 = (1.0/(n-1)) * torch.ones_like(bndiff2) * dbnvar\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "\n",
    "# bndiff2 = bndiff **2\n",
    "# cmp('bndiff', dbndiff, bndiff)\n",
    "\n",
    "#the derivative of x**2 = 2x\n",
    "\n",
    "dbndiff += (2 *bndiff) * dbndiff2\n",
    "cmp('bndiff', dbndiff, bndiff) \n",
    "\n",
    "# bndiff = hprebn - bnmeani\n",
    "print(f\"shape of bndiff {bndiff.shape} | shape of hprebn {hprebn.shape} | shape of bnmeani {bnmeani.shape}\")\n",
    "#the broadcasting is happened during the forward pass, So we have to sum during the  back propagation\n",
    "dhprebn = dbndiff.clone()\n",
    "dbnmeani = (-torch.ones_like(bndiff) * dbndiff).sum(0, keepdims= True) \n",
    "\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "\n",
    "# bnmeani= 1/n*hprebn.sum(0,keepdim = True) #(hprebn.sum(0,keepdim = True)/n)\n",
    "# Since the values are squashed during the forward passs , we have to apply broadcasting during the back propagation\n",
    "\n",
    "dhprebn += (1.0/n) * (torch.ones_like(hprebn) * dbnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8644e7a-6b1d-4e31-b1b7-6d019a6fcc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dhprebn torch.Size([32, 64])  \n",
      "shape of embcat torch.Size([32, 30]) \n",
      "shape of W1 torch.Size([30, 64])\n",
      "shape of b1 torch.Size([64])\n",
      "embcat          | exact: True  | approximate True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "#liner layer 1 \n",
    "# hprebn = embcat @ W1 + b1 #hidden layer pre activation\n",
    "\n",
    "print(f\"Shape of dhprebn {dhprebn.shape}  \\nshape of embcat {embcat.shape} \\nshape of W1 {W1.shape}\\\n",
    "\\nshape of b1 {b1.shape}\")\n",
    "#always remember the matrix derivatives Tips\n",
    "dembcat = dhprebn @ W1.T\n",
    "cmp('embcat', dembcat, embcat)\n",
    "\n",
    "dW1 = embcat.T @ dhprebn\n",
    "cmp('W1', dW1, W1)\n",
    "\n",
    "db1 = dhprebn.sum(0)\n",
    "cmp('b1', db1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6047c0e-c946-4510-a71b-148595d272c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of embcat torch.Size([32, 30]) | shape of emb torch.Size([32, 3, 10])\n",
      "emb             | exact: True  | approximate True  | maxdiff: 0.0\n",
      "shape of emb torch.Size([32, 3, 10]) | shape of C torch.Size([27, 10]) | shape of Xb torch.Size([32, 3])\n",
      "C               | exact: True  | approximate True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# emb = C[Xb] # embed the characters into the vector\n",
    "# embcat = emb.view(emb.shape[0], -1) #Concatenate the vectors\n",
    "\n",
    "print(f\"shape of embcat {embcat.shape} | shape of emb {emb.shape}\")\n",
    "#shape of embcat torch.Size([32, 30]) | shape of emb torch.Size([32, 3, 10])\n",
    "# 30 should be viewed as 3 x 10 , Can be achieved using view\n",
    "# We can pass tuple as a input , Since embcat derived from emb , It will work\n",
    "demb = dembcat.view(emb.shape)\n",
    "\n",
    "cmp('emb', demb, emb)\n",
    "\n",
    "# cmp('C', dC, C)\n",
    "print(f\"shape of emb {emb.shape} | shape of C {C.shape} | shape of Xb {Xb.shape}\")\n",
    "# we have to route the derivatives to each of the embeddingd of each letters in the XB\n",
    "# the letters may be repetetive so they should add on if they exist more than once\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k,j]\n",
    "\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f95ca86-6c05-46f1-9b9c-287e3d5c9630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5, 12],\n",
       "        [ 0,  0, 26],\n",
       "        [26,  1,  3],\n",
       "        [20, 19,  8],\n",
       "        [ 0,  0, 26]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7bfa66d-15da-4713-aced-4d4e63d3f520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
       "         0.0791,  0.9046], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5d7dd10-7839-4c9b-aa6d-2774843ca83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0055,  0.0030,  0.0030,  0.0047,  0.0012,  0.0019,  0.0004, -0.0013,\n",
       "         0.0035,  0.0029], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demb[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb63349a-1633-4e67-bbc8-0e003a98d0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
       "         0.0791,  0.9046], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d4494c5-b8b5-4fbc-b38b-459e045d9a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4946985244750977 diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7dc7d1fb-3529-406e-94c5-a9b70e2f8361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.4947, grad_fn=<NllLossBackward0>),\n",
       " tensor(3.4947, grad_fn=<NegBackward0>))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fast, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b958fa6-d6f4-43fd-8ec7-029dbcedfb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate True  | maxdiff: 5.122274160385132e-09\n"
     ]
    }
   ],
   "source": [
    "#backward pass \n",
    "# Remember Dont forget to practise calculas for back propagation\n",
    "dlogits = F.softmax(logits, 1)\n",
    "dlogits[range(n), Yb] -= 1\n",
    "dlogits /= n\n",
    "\n",
    "cmp('logits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee90fbcd-6ceb-4bc4-a3f1-67694343122f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape , Yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffd70420-a5cd-4fde-81f9-7b4cc5c4c117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0208, 0.0176, 0.0329, 0.0167, 0.0515, 0.0264, 0.0737, 0.0517, 0.0317,\n",
       "        0.0293, 0.0352, 0.0405, 0.0454, 0.0580, 0.0180, 0.0269, 0.0248, 0.0193,\n",
       "        0.0320, 0.1183, 0.0428, 0.0265, 0.0318, 0.0479, 0.0311, 0.0215, 0.0275],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c56aec6-7d15-4ea2-a55b-30e7baae698c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0208,  0.0176,  0.0329,  0.0167,  0.0515,  0.0264,  0.0737,  0.0517,\n",
       "         0.0317,  0.0293,  0.0352,  0.0405, -0.9546,  0.0580,  0.0180,  0.0269,\n",
       "         0.0248,  0.0193,  0.0320,  0.1183,  0.0428,  0.0265,  0.0318,  0.0479,\n",
       "         0.0311,  0.0215,  0.0275], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0] * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40a7a7e3-628e-40c0-806e-31aa530533d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.8626e-09, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da2f9e5c-48ee-44c4-bbb2-c1c94af32970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23905f158d0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (6,6))\n",
    "plt.imshow(dlogits.detach(), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d750f6cb-41b1-4589-a2fa-29b86e3ef645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) /\\\n",
    "torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bec6ed9c-7a15-42e2-9f0f-2d5e74107477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0836c26-4bba-4f08-8895-81b69e5c7d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]),\n",
       " torch.Size([1, 64]),\n",
       " torch.Size([1, 64]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dhprebn.shape, bngain.shape, bnvar_inv.shape, dbnraw.shape, dbnraw.sum(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24c7d3ff-7492-479f-b138-846f8460540c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.7214\n",
      "  10000/ 200000: 2.2107\n",
      "  20000/ 200000: 2.5586\n",
      "  30000/ 200000: 2.4530\n",
      "  40000/ 200000: 2.2200\n",
      "  50000/ 200000: 2.1479\n",
      "  60000/ 200000: 2.2120\n",
      "  70000/ 200000: 1.6477\n",
      "  80000/ 200000: 2.5439\n",
      "  90000/ 200000: 2.0995\n",
      " 100000/ 200000: 2.1476\n",
      " 110000/ 200000: 1.9903\n",
      " 120000/ 200000: 2.1616\n",
      " 130000/ 200000: 2.3060\n",
      " 140000/ 200000: 2.2308\n",
      " 150000/ 200000: 2.1194\n",
      " 160000/ 200000: 2.2082\n",
      " 170000/ 200000: 1.7720\n",
      " 180000/ 200000: 2.0107\n",
      " 190000/ 200000: 2.2511\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "\n",
    "  # kick off optimization\n",
    "  for i in range(max_steps):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    # Linear layer\n",
    "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "    # BatchNorm layer\n",
    "    # -------------------------------------------------------------\n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    # -------------------------------------------------------------\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "      p.grad = None\n",
    "    #loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "    # manual backprop! #swole_doge_meme\n",
    "    # -----------------\n",
    "    dlogits = F.softmax(logits, 1)\n",
    "    dlogits[range(n), Yb] -= 1\n",
    "    dlogits /= n\n",
    "    # 2nd layer backprop\n",
    "    dh = dlogits @ W2.T\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(0)\n",
    "    # tanh\n",
    "    dhpreact = (1.0 - h**2) * dh\n",
    "    # batchnorm backprop\n",
    "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "    # 1st layer\n",
    "    dembcat = dhprebn @ W1.T\n",
    "    dW1 = embcat.T @ dhprebn\n",
    "    db1 = dhprebn.sum(0)\n",
    "    # embedding\n",
    "    demb = dembcat.view(emb.shape)\n",
    "    dC = torch.zeros_like(C)\n",
    "    for k in range(Xb.shape[0]):\n",
    "      for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k,j]\n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "    # -----------------\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "    for p, grad in zip(parameters, grads):\n",
    "      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "  #   if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "  #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b8dc7081-970a-4b3b-9b55-0f98b8eea807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful for checking your gradients\n",
    "# for p,g in zip(parameters, grads):\n",
    "#   cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "766f9ff7-11ec-4726-819b-dc2d345966d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd315f8a-b52d-4da3-954b-80920b8c161a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0705130100250244\n",
      "val 2.124890089035034\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0394630c-23fe-4f57-a5a4-bc5bed80541d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mora.\n",
      "kayah.\n",
      "see.\n",
      "mad.\n",
      "ryla.\n",
      "remmaniekdra.\n",
      "grazelyn.\n",
      "elin.\n",
      "shi.\n",
      "jen.\n",
      "eden.\n",
      "sana.\n",
      "arleigh.\n",
      "malkia.\n",
      "noshubergihimiel.\n",
      "kinleenelionnie.\n",
      "casube.\n",
      "ged.\n",
      "ryyah.\n",
      "fael.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # ------------\n",
    "      # forward pass:\n",
    "      # Embedding\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
    "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "      hpreact = embcat @ W1 + b1\n",
    "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + b2 # (N, vocab_size)\n",
    "      # ------------\n",
    "      # Sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cda8dd-f31f-4573-8c5e-a64ca5167b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
