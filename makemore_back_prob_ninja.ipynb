{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fba3f4d3-8620-4b45-8048-c3736ae6e0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb197ce6-d904-4448-9bf9-42c8d102d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "if Path('names.txt').exists():\n",
    "    words = open('names.txt', 'r').read().splitlines()\n",
    "else:\n",
    "    req = requests.get(r'https://raw.githubusercontent.com/karpathy/makemore/master/names.txt')\n",
    "    with open('names.txt', 'wb') as f:\n",
    "        f.write()\n",
    "    words = open('names.txt','r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59323a6c-429b-4031-a70f-44df451bcc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a973c3c-b120-4028-bd6f-be1458754e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf2feccd-1a49-43bc-8b03-b868c5548c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1e750c59-b11b-4858-aafc-77d78b30f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function which we will use later when copamring Manual Gradients to Pytorch Gradients\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f\"{s:15s} | exact: {str(ex):5s} | approximate {str(app):5s} | maxdiff: {maxdiff}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af444748-b7a3-4b9f-996f-9a5f6133ee27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Themes: \n",
      "   chesterish\n",
      "   grade3\n",
      "   gruvboxd\n",
      "   gruvboxl\n",
      "   monokai\n",
      "   oceans16\n",
      "   onedork\n",
      "   solarizedd\n",
      "   solarizedl\n"
     ]
    }
   ],
   "source": [
    "!jt -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad472e38-0e1d-4fd9-bb5d-330411fbf19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='onedork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b1fea37-5021-4dee-9d40-6990fa36f0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 64\n",
    "\n",
    "g = torch.Generator().manual_seed(42)\n",
    "C = torch.randn((vocab_size, n_embd), generator = g)\n",
    "\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator= g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden, generator = g) * 0.1  # using b1 just for fun, it's useless because of BN\n",
    "\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator = g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator = g) * 0.1\n",
    "\n",
    "#BatchNorm Parameters\n",
    "bngain = torch.randn((1, n_hidden), generator = g)*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden), generator = g) * 0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "733b5ec5-723d-4e32-b94c-257488925e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size #a shorter variable for convinence\n",
    "#constrcut a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0],(batch_size,), generator = g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a96e3615-cec5-4a73-9193-e2a4264f4952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4786, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Forward pass,k \"chunckated into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into the vector\n",
    "embcat = emb.view(emb.shape[0], -1) #Concatenate the vectors\n",
    "\n",
    "#liner layer 1 \n",
    "hprebn = embcat @ W1 + b1 #hidden layer pre activation\n",
    "#Batch Norm Layer\n",
    "bnmeani= 1/n*hprebn.sum(0,keepdim = True) #(hprebn.sum(0,keepdim = True)/n)\n",
    "bndiff = hprebn - bnmean\n",
    "bndiff2 = bndiff **2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim= True) #note : Bessel's Correction (dividing by n-1 , not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**0.5\n",
    "bnraw = bndiff - bnvar_inv\n",
    "hpreact = bngain * bnraw +bnbias\n",
    "\n",
    "#Non Linearity\n",
    "h = torch.tanh(hpreact) #hidden layer\n",
    "\n",
    "#linear layer 2\n",
    "logits = h @ W2 + b2 #Output Layer\n",
    "\n",
    "#cross entropy loss (same as F.cross_Entropy loss)\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes #subtract the max for numerical stability refer the previous notebooks \n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims = True)\n",
    "counts_sum_inv = counts_sum ** -1  # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "#pytorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06805db2-3af2-4e9b-9ea9-209606f6cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ea54ba48-79b7-4a00-8fb6-70f55f3560e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs shape torch.Size([32, 27])\n",
      "Calculating the gradients for ech parameters or steps which will be used in back propagation\n",
      "\n",
      "the gradients that need to be updated for a step are calculated by finding the derivative from the succeeding step and\n",
      "will be stored in the parameter.grad \n",
      " \n",
      "since the most part of the logprobs which has the shape of torch.Size([32, 27]) is gonna be zero     \n",
      "Because only the logprobs[range(n), Yb] of shape torch.Size([32]) will be taken into consideration\n",
      "      and loss of other logits or logprobs will be derviatively - d/dx zero\n",
      "\n",
      "-----Implementing the derivative-----\n",
      "logprobs        | exact: True  | approximate True  | maxdiff: 0.0\n",
      "\n",
      "\n",
      "derivative of probs.log() is equivalent to log(x) which is 1/x and need to implement chain rule      \n",
      "that is wee need to find the derivative of probs which is dlogprobs\n",
      "Compare the gradients probs.grad[range(n), Yb] and dprobs[range(n), Yb]\n",
      "probs           | exact: True  | approximate True  | maxdiff: 0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cmp('logprobs', dlogprobs, logprobs)\n",
    "print('logprobs shape',logprobs.shape)\n",
    "#calcualte the dlogprobs \n",
    "#example loss = -(a + b + c) /3 where 3 is the n \n",
    "#dloss/da = -1/3 where b and c are constatnts and will become zero so the derivative loss for n logits can be written as -1/n\n",
    "print(f\"Calculating the gradients for ech parameters or steps which will be used in back propagation\\n\")\n",
    "print(\"the gradients that need to be updated for a step are calculated by finding the derivative from the succeeding step and\\\n",
    "\\nwill be stored in the parameter.grad \\n \")\n",
    "print(f\"since the most part of the logprobs which has the shape of {logprobs.shape} is gonna be zero\\\n",
    "     \\nBecause only the logprobs[range(n), Yb] of shape {logprobs[range(n), Yb].shape} will be taken into consideration\\n \\\n",
    "     and loss of other logits or logprobs will be derviatively - d/dx zero\")\n",
    "\n",
    "print(\"\\n-----Implementing the derivative-----\")\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "#1.0/n is the equivalent of dloss/dlogprobs\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "print(\"\\n\")\n",
    "\n",
    "# cmp('probs', dprobs, probs)\n",
    "# now find the gradient of probs which will the derivative from the logprob step\n",
    "print(f\"derivative of probs.log() is equivalent to log(x) which is 1/x and need to implement chain rule\\\n",
    "      \\nthat is wee need to find the derivative of probs which is dlogprobs\")\n",
    "print(\"Compare the gradients probs.grad[range(n), Yb] and dprobs[range(n), Yb]\")\n",
    "dprobs = (1.0/probs) * dlogprobs\n",
    "cmp('probs',dprobs,probs)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f5b30694-5f35-48fa-b6e0-0ac54b42ee2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9789, -0.4115, -1.9931, -2.0392, -0.6188, -0.3112, -0.9464, -3.1652,\n",
       "        -1.6925, -0.5919, -1.1059, -1.2846, -0.8153, -3.5235, -1.0352, -2.2519,\n",
       "        -0.4642, -4.3114, -1.9340, -2.4587, -0.5418, -0.2978, -3.1657, -1.4804,\n",
       "        -0.4597, -2.0392, -1.2553, -0.1862, -0.6654, -0.2453, -0.6218, -1.0468])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.grad[range(n), Yb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "28202500-bbcd-4e93-988f-da5e8985f7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9789, -0.4115, -1.9931, -2.0392, -0.6188, -0.3112, -0.9464, -3.1652,\n",
       "        -1.6925, -0.5919, -1.1059, -1.2846, -0.8153, -3.5235, -1.0352, -2.2519,\n",
       "        -0.4642, -4.3114, -1.9340, -2.4587, -0.5418, -0.2978, -3.1657, -1.4804,\n",
       "        -0.4597, -2.0392, -1.2553, -0.1862, -0.6654, -0.2453, -0.6218, -1.0468],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dprobs[range(n), Yb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7b7bb739-7667-44c6-8685-707ffb14ec8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2599],\n",
       "        [-0.3763],\n",
       "        [-0.2715],\n",
       "        [-0.1683],\n",
       "        [-0.2736],\n",
       "        [-0.3112],\n",
       "        [-0.3070],\n",
       "        [-0.3149],\n",
       "        [-0.3927],\n",
       "        [-0.4288],\n",
       "        [-0.2620],\n",
       "        [-0.2978],\n",
       "        [-0.2364],\n",
       "        [-0.1683],\n",
       "        [-0.2216],\n",
       "        [-0.2254],\n",
       "        [-0.3198],\n",
       "        [-0.1823],\n",
       "        [-0.3176],\n",
       "        [-0.2977],\n",
       "        [-0.1683],\n",
       "        [-0.2978],\n",
       "        [-0.3288],\n",
       "        [-0.1683],\n",
       "        [-0.3726],\n",
       "        [-0.1683],\n",
       "        [-0.2816],\n",
       "        [-0.1862],\n",
       "        [-0.2729],\n",
       "        [-0.2453],\n",
       "        [-0.3128],\n",
       "        [-0.2908]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_sum_inv.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2283206c-e87b-45d1-8e48-5e26b00c645a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
